---
title: "Final Paper"
author: "STOR 320.01 Group 13"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(27) #DO NOT CHANGE THIS SEED IT IS NEEDED FOR REPRODUCABILITY SO OUR DATA WILL MAKE SENSE WITH THE WORDS WE USE PLEASE DONT CHANGE KEEP AT 27 PLEASE GOD KEEP AT 27
library(tidyverse)
library(patchwork)
library(rje)
library(caret)
library(kableExtra)
library(Boruta)
library(plotly)
library(randomForest)
library(formattable)
library(class)
library(rgl)
library(misc3d)
library(htmlwidgets)
library(htmltools)
library(knitr)
library(figpatch)
options(rgl.useNULL = TRUE)
knitr::opts_chunk$set(fig.align = "center")
```

# INTRODUCTION

|      League of Legends (LoL) is a popular multiplayer video game known for its ever-changing gaming strategies and various champions (characters that can be played). A foundation aspect of LoL is the tier system, which categorizes champions based on their strength and popularity in a game. In this paper, we explore the world of LoL meta and analyze the data to understand how champions are classified and how temporary changes affect their status.

|      In this paper, we address two analysis questions. First, to what extent can we reproduce the tier system from MetaSRC by using clustering methods? To determine whether our groups reproduce the same MetaSRC's tiers, we examined a dataset of LoL statistics and group champions selected by their stats. This will enable us to better understand how dependable and consistent the tier system is. Second, is there a statistically significant difference in the accuracies of predicting tiers when “temporary” champions are classified? Temporary champions are individuals who are missing 1 or more patches. Our goal is to determine whether there is a significant difference in their meta by comparing their stats to those of other champions. This will help us understand how temporary champions function in the competitive environment.

|      We web scraped all the data directly from metasrc.com/lol which contains stats from various League of Legends games played across many servers, including the EU/NA/KR and others. This data contains pick rates, champion victory rates, and more giving us an idea of the meta. Based on these statistics, we used RStudio to perform data analysis and looked for patterns.

|      Our analysis is important to the gaming community, specifically coaches, players, and LoL enthusiasts. Understanding the tier system and how to play temporary champions helps the players. Additionally, looking at the meta can improve our understanding of the game and help us make more informed decisions about selecting the best champion based on the current meta. Throughout this paper, we go into further detail about the data we used, the results, and the conclusions of our findings. We believe this study will aid in a clearer understanding of the LoL meta and aid LoL players who want to optimize their player selection.


# DATA

|      Our dataset was obtained by web scraping data from MetaSRC which is a database of various game stats. Our data was based on season 13 of League of Legends and contained 5637 observations of various statistics from patches 1 through 24 excluding 2 which was excluded from the database for game development reasons. These statistics are included in the table below.

```{r Dataset, echo = F}

leaguedf <- read_csv('../data_sets/S13LeagueOfLegendsData.csv', 
                      col_types=c('c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'c'), 
                      col_names=c('rowno', 'Name', 'Class', 'Role', 'Tier', 'Score', 'Trend', "WinRate", "RoleRate", "PickRate", "BanRate", 'KDA', 'Patch'), skip=1) %>%
  column_to_rownames("rowno") %>% 
  mutate(PickBanRate = PickRate + BanRate, 
         Patch = as.numeric(str_replace(Patch, '(.*?)_(.*?)', '')), 
         Role = str_to_title(Role))
leaguedf$Tier = as.factor(leaguedf$Tier) %>%
  fct_relevel(c("God", "S", "A", "B", "C", "D"))

head(leaguedf, 5) %>% kbl() %>%
  kable_classic(full_width = F, html_font = "cambria")
```

|      The Name variable shows the name of the character whose statistics are being shown with the class categorical variable describing the classifications of characters in the game based on the champions in-game stats. The Role variable describes the playstyle that a champion is usually played with in a game. Some possibilities for roles in the game include Top, Mid, Jungle, Support, and ADC. A good metric for determining how good a character is includes KDA which is calculated as KDA = (kills + assists)/ deaths. WinRate is the ratio of wins to losses with a selected character. PickRate is what percentage of games that character is picked and helps indicate the popularity of a champion.  BanRate is the percentage of games in which that character is banned in the pre-game champion selection process.
|      Tier shows how good MetaSRC grades the character to be with God being the best and D being the worst. This metric was especially important for both of our analysis questions as we tried to use the data to see how accurate their tiers are at evaluating performance and see if our own tiers made through clustering are accurate. The variables Score and Trend were not used in our analysis, but come from MetaSRC as a way of continuously grading champions, as opposed to the discrete grade from trend.

```{r Data Plot, echo = F}

sumdf <- leaguedf %>%
  group_by(Tier) %>%
  summarize(avg = mean(KDA), avgpick = mean(PickRate), avgban = mean(BanRate), avgwin = mean(WinRate)) %>%
  ungroup()


p1 <- sumdf %>% 
  ggplot() + 
  geom_col(mapping = aes(x = Tier, y = avg), fill = "#44AA99") + 
  coord_cartesian(ylim = c(2, 2.5)) + 
  labs(title = "KDA", x = "Tier", y = "Mean") + 
  theme_minimal()

p2 <- sumdf %>%
  ggplot() + 
  geom_col(mapping = aes(x = Tier, y = avgpick), fill = "#44AA99") + 
  labs(title = "PickRate", x = "Tier", y = "Mean") + 
  theme_minimal()

p3 <- sumdf %>%
  ggplot() + 
  geom_col(mapping = aes(x = Tier, y = avgban), fill = "#44AA99") + 
  labs(title = "BanRate", x = "Tier", y = "Mean") + 
  theme_minimal()

p4 <- sumdf %>%
  ggplot() + 
  geom_col(mapping = aes(x = Tier, y = avgwin), fill = "#44AA99") + 
  labs(title = "WinRate", x = "Tier", y = "Mean") + 
  coord_cartesian(ylim = c(0.48, 0.52)) + 
  theme_minimal()

p1 + p4 + p2 + p3 + plot_layout(ncol = 2, nrow = 2, axis_titles = "collect") + plot_annotation("Statistics grouped by Tier", caption = "All y axes are on different scaled, but they\n all represent means of a certain statistic.")
```

|      This figure shows how generally the tiers are correlated with characters that perform better in the game. This is because KDA is often used as a metric for determining how well a specific character did in a game. However, this plot does show discrepancies as in this case, the S rank tier has a higher KDA than the god tier which does not . This is precisely the goal of our analysis to evaluate and perhaps create better tiers for characters.

# RESULTS

```{r Pallettes, echo = F}
KMeansPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00")
TierPalette <- c("#332288", "#117733", "#44AA99", "#88CCEE", "#DDCC77", "#CC6677")
```

## Part 1: Clustering Analysis


|      The first idea that this paper aims to investigate is to what extent MetaSRC tiers correspond to groups generated by K-means clustering. We specifically choose K-Means to compare MetaSRC tiers to as (heuristically) tiers should be centered around some point, if one wanted to know whether or not a champion was truly “God” tier, then one should have some understanding of where this means the champion stands in terms of statistical features (`PickRate`, `BanRate`, `WinRate`, …). K-means clustering works similarly to a K-nearest neighbors prediction algorithm, except the end goal is different. 

|      In K-means, the input is: the number of clusters that should be outputted ($k$), a distance metric (for example $d(x,y) = \sqrt{\sum_{i=1}^n x_i^2 + y_i^2}$ which is commonly known as Euclidean distance) , and the dataset without any target variables (the variable to be predicted). Then, the algorithm creates groups, $C_1, \ldots, C_k$, with the condition that:

  1) each observation in the dataset belongs to at least one group ($\bigcup_{i=1}^k C_i = D$ where $D$ is the dataset) and 
  2) each observation belongs to at most one group ($\forall i \neq j: C_i \cap C_k = \emptyset$). 
  
|      In order to do this most efficiently, it begins by randomly sampling the dataset and assigning each observation a value from $1$ to $k$. It then performs a loop where it minimizes the average distance to the centroid of the group (a point which is the mean of each variable in the cluster). As a result of this, it should be noted that the K-means algorithm only accepts numeric variables, categorical variables in our example (Role and Class are both used) were one-hot encoded to solve this, before the data is then normalized in order to reduce potential bias from variables with larger standard deviations. Our method uses all variables present in the dataset excluding Name, PickBanRate, Tier, Score, and Trend. We chose to remove these variables as they are either unique identifiers, not representative of collected data (Tier, Score, and Trend are all from metaSRC directly), or repetitive (PickBanRate is `PickRate` + `BanRate`). We also numbered the clusters in terms of descending mean `RoleRate`, as that can be seen to follow some sort of a similar trend with metaSRC tiers. This helps in terms of visualization and understanding the data, as you can see below.

```{r KMeans, fig.height = 10, fig.width = 10, warning = F, echo = F}
#Cluster Analysis with K-Means


#Step 1: Normalize Data:
#First drop icky Vars and then Dummy encode Name, Class, and Role
#This is a high dimensional Data set

Normaldf <- leaguedf %>%
  select(-c(Tier, Score, Trend, PickBanRate, Name)) %>%
    pivot_wider(names_from = Role,
              values_from = Role,
              values_fn = function(x) 1,
              values_fill = 0) %>%
    mutate(Class = paste("Class: ", Class, sep = '')) %>%
    pivot_wider(names_from = Class,
                values_from = Class,
                values_fn = function(x) 1,
                values_fill = 0) %>%
  mutate(
    WinRate = (WinRate - mean(WinRate))/sd(WinRate),
    RoleRate = (RoleRate - mean(RoleRate))/ sd(RoleRate),
    PickRate = (PickRate - mean(PickRate)) / sd(PickRate),
    BanRate = (BanRate - mean(BanRate)) / sd(BanRate),
    KDA = (KDA - mean(KDA)) / sd(KDA),
    Patch = (Patch -mean(Patch)) / sd(Patch)
  )
  
#Step 2: Clusterize the Data

data <- kmeans(Normaldf, centers = 6, nstart = 25)

leaguedf$Cluster = as.factor(data$cluster)

#Reproducibility for Graphing purposes

ordering <- leaguedf %>%
  group_by(Cluster) %>%
  summarize(RoleRate = mean(RoleRate)) %>%
  arrange(desc(RoleRate)) %>%
  mutate(transformation = row_number())

transform <- function (x) {
  temp <- ordering %>%
    filter(Cluster == x)
  return (temp[[1, 3]])
}

leaguedf$Cluster <- sapply(leaguedf$Cluster, transform)

leaguedf <- leaguedf %>%
  mutate(Cluster = as.factor(Cluster))

plot1a <- leaguedf %>%
  ggplot() + 
  geom_point(mapping = aes(x = KDA, y = WinRate, color = Cluster), size = 0.75, alpha = 0.4) + 
  labs(x = "KDA", y = "Win Rate") + 
  theme_minimal()+ 
  theme(legend.position = "none") + 
  scale_color_manual(values = KMeansPalette)

plot1b <- leaguedf %>%
  ggplot() +
  geom_point(mapping = aes(x = PickBanRate, y = WinRate, color = Cluster), size = 0.75, alpha = 0.4) + 
  labs(x= "Pick/Ban Rate", y= "") + 
  theme_minimal()+ 
  scale_color_manual(values = KMeansPalette)+ 
  theme(legend.position = "bottom")+
     guides(color = guide_legend(override.aes = list(size = 3) ) )

plot1c <- leaguedf %>%
  ggplot() + 
  geom_boxplot(mapping = aes(x = Role, y = RoleRate, color = Cluster), lwd = 0.5) +
    labs(x = "Role", y = "Role %") + 
  theme_minimal() + 
  scale_color_manual(values = KMeansPalette)+ 
  theme(legend.position = "none")
design <- "
12
12
12
12
33
33
33
33
44"

KMeans <- wrap_elements(plot1a + plot1b + plot1c + guide_area() + 
  plot_layout(design = design, guides = "collect") &
  plot_annotation(title = "K Means"))


plot1a <- leaguedf %>%
  ggplot() + 
  geom_point(mapping = aes(x = KDA, y = WinRate, color = Tier), size = 0.75, alpha = 0.4) + 
  labs(x = "KDA", y = "Win Rate") + 
  theme_minimal()+ 
  scale_color_manual(values = TierPalette)+ 
  theme(legend.position = "none")

plot1b <- leaguedf %>%
  ggplot() +
  geom_point(mapping = aes(x = PickBanRate, y = WinRate, color = Tier), size = 0.75, alpha = 0.4) + 
  labs(x= "Pick/Ban Rate", y= "") + 
  theme_minimal()+ 
  scale_color_manual(values = TierPalette)+ 
  theme(legend.position = "bottom")+
     guides(color = guide_legend(override.aes = list(size = 3)))

plot1c <- leaguedf %>%
  ggplot() + 
  geom_boxplot(mapping = aes(x = Role, y = RoleRate, color = Tier), lwd= 0.5) + 
  labs(x = "Role", y = "Role %") + 
  scale_color_manual(values = TierPalette)+ 
  theme_minimal() + 
  theme(legend.position = "none")


Meta_Tiers <- wrap_elements(plot1a + plot1b + plot1c + guide_area() + 
  plot_layout(design = design, guides = "collect") &
  plot_annotation(title = "Meta SRC Tier"))


(KMeans | Meta_Tiers) & plot_annotation(title = "Cluster Analysis") & 
  theme(plot.title = element_text(hjust = 0.5, size = 15, face = 'bold')) 
```



```{r, fig.height = 10, fig.width = 10, eval = F, echo = F}
#Hierarchical Clustering
HCluster <- hclust(dist(Normaldf))

plot(HCluster, xlab = '', sub = '', cex = .9) #Dendrogram!!!
```


```{r, fig.height = 10, fig.width = 10, eval  = F, echo = F}

leaguedf$HClust <- as.factor(cutree(HCluster, 5))

plot1a <- leaguedf %>%
  ggplot() + 
  geom_point(mapping = aes(x = KDA, y = WinRate, color = HClust), size = 0.6, alpha = 0.8) + 
  labs(x = "KDA", y = "Win Rate") + 
  theme_minimal()+ 
  theme(legend.position = "none")

plot1b <- leaguedf %>%
  ggplot() +
  geom_point(mapping = aes(x = PickBanRate, y = WinRate, color = HClust), size = 0.6, alpha = 0.8) + 
  labs(x= "Pick/Ban Rate", y= "Win Rate") + 
  theme_minimal()+ 
  theme(legend.position = "right")+
     guides(color = guide_legend(override.aes = list(size = 3) ) )

plot1c <- leaguedf %>%
  ggplot() + 
  geom_boxplot(mapping = aes(x = Role, y = RoleRate, color = HClust), lwd = 0.5) +
    labs(x = "Role", y = "Role %") + 
  theme_minimal() + 
  theme(legend.position = "none")



((plot1a | plot1b) / plot1c )& 
  plot_layout(guides = "collect") &
  plot_annotation(title = "Hierarchical Cluster Analysis")  & theme(plot.title = element_text(hjust = 0.5, size = 15, face = 'bold'))
```


```{r, eval = F, echo = F}
leaguedf %>%
  rename(`Hierarchical Cluster` = HClust) %>%
  group_by(`Hierarchical Cluster`) %>%
  summarize(
            `Mean Win Rate` = mean(WinRate),
            `Mean PB Rate` = mean(PickBanRate),
            `Mean Role %` = mean(RoleRate),
            `Mean KDA` = mean(KDA),
            `Median Patch` = median(Patch),
            `Number of Champs` = n_distinct(Name)) %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Times New Roman")
```

```{r, message = F, echo = F, eval = T, echo = F}
Propdf <- leaguedf %>%
  group_by(Tier, Cluster) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  group_by(Tier) %>%
  mutate(Proportion = count / sum(count)) %>%
  arrange(desc(Proportion))

heatplot <- Propdf %>%
  ggplot() +
  geom_tile(mapping = aes(x = Tier, y = Cluster, fill = Proportion)) & 
  theme_minimal()

heatplot & plot_annotation(title = "Similarity Heatmap")
```

|      In our example, from the K-means algorithm, we can see that the overall similarities between most of the tiers (compared to clusters) are minimal. However, it is interesting to note that the similarities between cluster $1$ and God tier, as well as cluster $6$ and D tier, are much higher than any other similarity between the two groups. This helps us understand that these two tiers are closer to being well-defined within the feature space, and we will likely see that our model will have a higher accuracy when it comes to predicting these tiers as opposed to the others. Another interesting takeaway is the way that `RoleRate` has a much larger gap between the clusters as opposed to the metaSRC tiers, which could be a sign that further investigation should look at subsets of roles, and possibly patch as well. 
|      From our similarity heatmap, we can see that God and D tier have high amounts of overlap with the clusters with the highest mean RoleRate (1) and lowest mean RoleRate (6) respectively. This is interesting, because it tells us that RoleRate can help us to differentiate from the highest and lowest tiers, but not much beyond that. It also tells us that our model in the next part will likely perform better on God and D tiers then it does on the other 4 tiers, because these tiers have more a natural, data based boundaries.
	
	
## Part 2: Classification Exploration

|      For the second question, we want to understand how our model performs based on temporary and nontemporary champions. We define a temporary champion to be any champion who is not present in a specific role for all 23 patches (`df %>% group_by(Name, Role) %>% tally() %>% filter(n < 23)`). For example, Ivern is present in the Jungle role for 23 patches, so these observations would be considered nontemporary, but Ivern is present in the Top role for 22 patches, so only these 22 patches would be considered temporary.

```{r TempChampsPlot, echo = F, message = F, warning = F}
tempchamps <- leaguedf %>%
  complete(nesting(Name, Role), Patch) %>% # This explicitly finds champions who were only played in a role significantly for less than all of the patches!
  filter(!complete.cases(.)) %>%
  count(Name, Role)

tempdf <- leaguedf %>%
  filter(Name %in% tempchamps$Name & complete.cases(.)) %>%
  group_by(Name, Role) %>%
  summarize(
    n = n(),
    invN = 1/n,
    meanWinRate = mean(WinRate),
    sdWinRate = sd(WinRate),
    meanPickBan = mean(PickBanRate),
    sdPickBan = sd(PickBanRate), 
    label = paste(Name, '\n', Role, sep = " ")
  ) %>%
  filter(n != 23) %>%
  distinct()

p1 <- tempdf %>%
  ggplot(mapping = aes(x = meanWinRate, y = meanPickBan, alpha = invN)) + geom_point() +
  geom_vline(xintercept = .50, color = 'red') + 
  coord_trans(x = 'log10', y = 'log10') + 
  labs(x = "Win Rate Average", y = "Pick Ban Rate Average", title = "Win Rate vs Pick Ban Rate\nFor Temporary Champions") + 
  scale_alpha_continuous(name = "Inverse of n", 
                         breaks = c(0.05, 0.06, 0.125, 1),
                         labels = c(23, 16, 8, 1),
                         range = c(0.1, 1)) + 
  theme(legend.position = "bottom")


p2 <- tempdf %>%
  ggplot() + 
  geom_histogram(mapping = aes(x = n)) + 
  labs(x = "Patches Present", y = "Count", title = "Distribution of Temporary Champions") + 
  theme(legend.position = "bottom")
p1area <- patchwork::area(t = 1, b = 9, l = 1, r = 1)
p2area <- patchwork::area(t = 1, b = 9, l = 2, r = 2)
lgndarea <- patchwork::area(t = 10, b = 10, l = 1, r = 2)
p1 + p2 + guide_area() + plot_layout(guides = "collect", design = c(p1area, p2area, lgndarea))
```

|      We want to investigate this question because temporary champions are one of the main reasons for tier lists to exist, meta champions that have high pick ban rates and sporatic win rates are often evaluated by casual players in order to understand whether or not they are viable. Our paper focuses on the possible difference between these two groups, is there a significant difference in the way that temporary champions are classified that would matter to a potential player. From the figure above, it's evident that there is no clear grouing for these champions, they are distributed randomly, which makes it an excellent test group because we arent testing just one small section of our model. 


### Feature Selection

|      First, the feature space should be reduced as the original dimensions of the dataset (around 200) are too high for a meaningful conclusion from reasonable computational power to be done from. To do this, this paper employs a two-step technique: A stepwise regression to determine the optimal number of features for each tier, and a Boruta model to determine which features are most important to the dataset for all six tiers. 

```{r, eval = F, echo = F}
PCAdf <- leaguedf %>%
  mutate(Class = paste("Class", Class, sep='_')) %>%
  pivot_wider(
    names_from = "Role",
    values_from = "Role",
    values_fn = function (x) 1,
    values_fill = 0
  ) %>%
  pivot_wider(
    names_from = "Class",
    values_from = "Class",
    values_fn = function (x) 1,
    values_fill = 0
  ) %>%
  select(-c(Score, Trend, Tier, Cluster, HClust, Patch, Name)) #i'm taking Patch out since it basically messes with the scaling. Patch is uniform, so variance is maximized when patch is maximized

PCA1 <- prcomp(PCAdf, center = T, scale = F)

```

```{r, eval = F, echo = F}

eigensum <- function(eigen) {
  answer <- vector(length = length(eigen)) 
  sum = 0
  for (i in 1:length(eigen)) {
    sum = sum + eigen[i]
    answer[i] = sum
  }
  return(answer)
}
eigen <- PCA1$sdev^2

eigFrame <- data.frame(dim = factor(1:length(eigen)), eig = eigen, sum = eigensum(eigen))



Principles <- PCA1$rotation %>%
  dimnames

PCACoef <- PCA1$rotation %>%
  as_tibble() %>%
  select(c(1,2,3))

eigFrame %>%
  ggplot() + 
  geom_bar(mapping = aes(y = eig, x = dim, fill = 'green'), stat = 'identity') + 
  geom_bar(mapping = aes(y = sum, x = dim, fill = 'blue'), stat = 'identity', alpha = 0.3, ) + 
  scale_fill_manual(values = c("green" = "green", "blue" = "blue"), labels = c( "Rolling Summation", "Eigenvalue")) + 
  labs(fill = "Value",x = "Dimension Number", y = "Eigenvalue") + 
  theme(legend.position = c(0.15, 0.95),
        legend.justification = "top")

```


```{r Stepwise Regression, fig.height = 10, fig.width = 10, warning =  F, echo = F}
Stepdf <- leaguedf %>%
  mutate(Class = paste("Class", Class, sep='_')) %>%
  pivot_wider(
    names_from = "Role",
    values_from = "Role",
    values_fn = function (x) 1,
    values_fill = 0
  ) %>%
  pivot_wider(
    names_from = "Class",
    values_from = "Class",
    values_fn = function (x) 1,
    values_fill = 0
  ) %>%
  select(-c(Score, Trend, Patch, Name, Cluster))
stepResults <- tibble(Tier = c('0'), results = c(list()), .rows = 0)
tiers <- unique(leaguedf$Tier)

for (tier in tiers) {
  tempStep <- Stepdf %>%
    mutate(Tier = Tier == tier)
  
  stepModel <- lm(Tier ~ ., data = tempStep)
    
  stepModel <- step(stepModel, direction = "both", trace=0)
    
  stepResults <- stepResults %>%
    add_row(Tier = tier, results = list(names(summary(stepModel)$aliased)[-1]))
}
plist = vector('list', length = length(tiers))
counter = 1
for (tier in levels(tiers)) {
  testVars <- stepResults %>% filter(Tier == tier) %>% select(results)
  testVars <- testVars$results[[1]]
  
  rSquaredFrame <- tibble(features = 1:length(testVars), rSquared = 0)
  tempStep <- Stepdf %>%
    mutate(Tier = Tier == tier)
  varlist <- c()
  for (var in testVars) {
    varlist <- append(varlist, var)
    tempStep2 <- select(tempStep, varlist, Tier)
    model <- lm(Tier ~ ., data = tempStep2)  
    
    rSquaredFrame[length(varlist), 2] = BIC(model)
  }
  
  p1 <- rSquaredFrame %>%
    ggplot(mapping = aes(x = features, y = rSquared)) + 
    geom_point(shape = 4, color = "red")  + 
    geom_line() + 
    geom_vline(xintercept = 8, color = 'blue') + 
    labs(x = "Number of Features", y= "BIC", title = tier) + 
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0.5))
  plist[[counter]] <- p1
  counter = counter + 1
}

plot <- Reduce('+', plist)
plot + plot_annotation(title = "BIC Analysis with Stepwise Regression faceted by Tier", subtitle = "Vertical Line at x=8") + plot_layout(axis_titles = "collect")
```

|      Stepwise regression works by fitting multiple linear models algorithmically in order to determine the point at which adding extra features to the model results in minimal payoffs, and removing features does not significantly harm the model. Specifically, it uses “BIC”, which is calculated by the formula $k \ln (n) - 2 \ln (\hat{L})$. Here, $\ln (\hat{L})$ is the natural logarithm of the maximum likelihood estimate, which corresponds to the idea of how likely you are to see the data based on the predictions from this model. In addition, $k$ is the number of features in the model, and $n$ is the number of observations the model is trained on. This is all to say that the value increases as the number of features increases and the model's "correctness" decreases, which is why we look for the lower values that the model finds.
|      From our stepwise regression, we can see that 8 features (denoted by the vertical line) is likely to be optimal, since any extra features beyond this point results in minimal payoffs. 


```{r Boruta Model, eval = T, echo = F}

#Feature Selection

boruta <- Boruta(Tier ~ ., data = select(leaguedf, -c(Score, Trend, Cluster, PickBanRate)))

plot(boruta, las = 2, cex.axis = 0.7)
```



|      Using our target feature count from the stepwise regression, we turn to a Boruta model. This technique creates ‘shadow variables’ which are randomly sorted copies of the original vectors in the dataframe, and then a `RandomForest` model is trained on the dataset with shadow features added. From this, it looks to see the importance of each feature (which is calculated with complex mathematical formulas such as Impurity) and compares the most important shadow feature with the importance of all the original features. In this way, we see whether or not any variable performs more effectively than essential randomness, which can help to determine how useful it will be in the end. Our Boruta model tells us that `WinRate`, `PickRate`, `Role`, and `BanRate` are the four most important variables, but since `Role` account for 5 features (Top, Jungle, Mid, ADC, Support) we have $8$ total features accounted for within these $4$ variables.


### Cross Validation

|      After choosing the features, we then train different models and use K-Fold cross validation to determine which is the best. K-Fold cross validation works by splitting the dataset into $k$ disjoint and equally sized samples. We then train each model on the dataset without sample $i$, before testing the model's metric on just sample $i$. This is repeated once for each sample. K-Fold cross validation is a powerful tool to understand how well the model captures the trends within the dataset, because it is trained on different samples each time, and tests every observation. We chose K-Fold cross validation over other methods like Leave-one-out cross validation (LOOCV) because of the computation power available for this project. Additionally, we choose $k=10$ samples for the same reason.  

```{r, echo = F}



featurecombs <- powerSet(c('WinRate', 'PickRate', 'Role', 'BanRate'))# 2^4 = 16 so 16 samples, for 3 models each with 20 runs gives a total of 960 trained models.
featurecombs <- featurecombs[-1]

leaguedf <- leaguedf %>% select(-c(PickBanRate, Cluster, Score, Trend)) 


```

```{r, echo = F}
leaguedf$sample <- sample(1:nrow(leaguedf), nrow(leaguedf))

b = nrow(leaguedf)/10

leaguedf <- leaguedf %>%
  mutate(sample = ceiling(sample/b))


crossdf <- data.frame(sample = rep(1:10, each = 15), svm_linear = 0, svm_radial = 0, ranger = 0, WinRate = 0, PickRate = 0, Role = 0, BanRate = 0)



fitControl <- trainControl(method='CV', 
                           number = 3, 
                           verboseIter=F)
```

```{r, eval = F, echo = F}
#Using a function here makes sure each model stays local and so we end up with a faster training. Consider using this idea elsewhere in the file!!!
trainandtest <- function (method, traindf, testdf, control) {
  if (method == "ranger" & length(colnames(traindf)) < 3) {
    model <- randomForest(
      Tier ~ .,
      data = traindf,
      mtry = 1
    )
  }
  else {
    model <- train(Tier ~ .,
                   data = traindf,
                   method = method,
                   trControl = control)
    testdf$pred <- predict(model, testdf) 
  }
  
  return(mean(testdf$Tier == testdf$pred))
}

featurecount <- 1
# Something is wrong here:
# MTRY For Ranger is auto failing with 1 feature. How do I set mtry low?
for (feature in featurecombs) {
  for (i in 1:10) {
    traindf <- leaguedf %>%
      filter(!(sample == i)) %>%
      select(all_of(feature), Tier)
    testdf <- leaguedf %>%
      filter(sample == i) %>%
      select(all_of(feature), Tier)
    
    #Train models, find the accuracy on the held out sample, and then record the data into the dataframe
    crossdf[((i-1) * 15) + featurecount, 4] <- trainandtest("ranger", traindf, testdf, fitControl)
    crossdf[((i-1) * 15) + featurecount, 2] <- trainandtest("svmLinear", traindf, testdf, fitControl)
    crossdf[((i-1) * 15) + featurecount, 3] <- trainandtest("svmRadial", traindf, testdf, fitControl)
    
    #Record the feature by putting 1 in the columns that have the feature present
    crossdf[((i-1) * 15) + featurecount, 5] <- "WinRate" %in% feature
    crossdf[((i-1) * 15) + featurecount, 6] <- "PickRate" %in% feature
    crossdf[((i-1) * 15) + featurecount, 7] <- "Role" %in% feature
    crossdf[((i-1) * 15) + featurecount, 8] <- "BanRate" %in% feature
    }
  featurecount <- featurecount + 1
  } 
```


```{r empty model training, echo = F, eval = F}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

for (i in 1:10) {
    traindf <- leaguedf %>%
      filter(!(sample == i)) %>%
      select(Tier)
    testdf <- leaguedf %>%
      filter(sample == i) %>%
      select(Tier)
    
    
    #Empty model
    p = traindf %>% count(Tier)
    mostcommon <- slice_max(p, n)
    percent = mean(testdf$Tier == mostcommon$Tier)
    crossdf <- crossdf %>%
      rbind(list(sample = i, svm_radial = percent, svm_linear = percent, ranger = percent, WinRate = 0, PickRate =0, BanRate =0, Role = 0))
}


```


```{r, echo  = F, message = F}
crossdf <- read_csv('crossdf.csv')
```

```{r, echo = F}
write_csv(crossdf, "crossdf.csv")
```

```{r CrossDF Conversion, echo = F, message = F}

convert_func <- function(WinRate, PickRate, BanRate, Role) {
  ans <- ''
  if (WinRate == 1) {
    ans <- paste(ans, "WinRate", sep = ' ')
  }
  if (PickRate == 1) {
    ans <- paste(ans, "PickRate", sep = ' ')
  }
  if (BanRate == 1) {
    ans <- paste(ans, "BanRate", sep = ' ')
  }
  if (Role == 1) {
    ans <- paste(ans, "Role", sep = ' ')
  }
  return (ans)
}

map4 <- function(v1, v2, v3, v4, func) {
  ans = vector(length = length(v1))
  for (i in 1:length(v1)) {
    ans[i] = func(v1[i], v2[i], v3[i], v4[i])
  }
  return (ans)
}
crossdf <- crossdf %>%
  pivot_longer(cols = c("svm_linear", "svm_radial", "ranger"), names_to = "Model", values_to = "Accuracy") %>%
  mutate(Feature = map4(WinRate, PickRate, BanRate, Role, convert_func)) %>%
  select(sample, Model, Accuracy, Feature) %>%
  na.omit() %>%
  group_by(Model, Feature) %>%
  summarize(sd = sd(Accuracy), Accuracy = mean(Accuracy)) %>%
  ungroup() %>%
  mutate(featurenum = str_count(Feature, pattern = ' '),
         Model = case_when(
           featurenum == 0 ~ "Empty Model",
           T~ Model))


crossdf$Feature <- crossdf$Feature %>%
  as.factor() 
```


```{r BarChart, fig.height = 10, fig.width=10, echo = F}

emptyline = geom_hline(yintercept = 0.2404, linetype = "dashed", color = "#61DC1E")

customlegend <- scale_fill_manual(values = c("svm_linear" = "#648FFF", "svm_radial" = "#DC267F","ranger" = "#FFB000"),
                                   labels = c("svm_linear" = "SVM Linear", "svm_radial" = "SVM Radial","ranger" =  "Ranger"))

p1 <- crossdf %>% filter(featurenum == 1) %>%
  na.omit() %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Feature, y = Accuracy, fill = Model), stat = "identity", position = "dodge") + 
  theme_minimal() + 
    labs(x = "Features", y = "Accuracy", title = "1 Feature Models") + ylim(0, 1) + customlegend+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + 
  theme(legend.position = "none") + emptyline
  
p2 <- crossdf %>% filter(featurenum == 2) %>%
  na.omit() %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Feature, y = Accuracy, fill = Model), stat = "identity", position = "dodge") + 
  theme_minimal() + 
    labs(x = "Features", y = "Accuracy", title = "2 Feature Models") + ylim(0,1  ) + customlegend+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + emptyline

p3 <- crossdf %>% filter(featurenum == 3) %>%
  na.omit() %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Feature, y = Accuracy, fill = Model), stat = "identity", position = "dodge") + 
  theme_minimal() + 
    labs(x = "Features", y = "Accuracy", title = "3 Feature Models") + ylim(0,1 ) + customlegend + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + emptyline

p4 <- crossdf %>% filter(featurenum == 4) %>%
  na.omit() %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Feature, y = Accuracy, fill = Model), stat = "identity", position = "dodge") + 
  theme_minimal() + 
    labs(x = "Features", y = "Accuracy", title = "4 Feature Models") + ylim(0,1) + customlegend+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + emptyline

p1 + p2 + p3 + p4 + plot_layout(guides = "collect", axis_titles = "collect", nrow = 2, ncol = 2) + plot_annotation(title = "Models and Mean Cross-Validated Accuracy", caption = "Horizontal Line represents the empty model accuracy, at 0.2404")



```



|      From the bar chart, we can see a few things: first, the inclusion of Pick- and WinRate is the most influential on the success of the model, which matches what our Boruta model found. Next, we can see that the models that contain the most variables are the most accurate, except that all models trained on WinRate, BanRate, and Role perform  worse than a SVM model trained on just PickRate. 

|      We next compare the models to the highest performing model to see if the increase in accuracy is statistically significant (we choose to give preference to SVM models over the Ranger, as the Ranger is less human readable than an SVM, and is often hard to determine if it’s overfitted. To do this, we perform a comparison with mean accuracy from all $10$ cross-validation folds, and use Welsh’s T-test to find P-Values at a statistically significant level (chosen to be $p = 0.05$). We use the following formula: $$t^* = \frac{\mu_0 - \mu_1}{\sqrt{\frac{\sigma_0^2}{n_0} + \frac{\sigma_1^2}{n_1}}}$$ 
|      Where $\sigma_0^2$, $n_0$, and $\mu_0$ are the variation, sample size, and mean accuracy for the ranger model with all 4 features, while $\sigma_1^2$, $n_1$, and $\mu_1$ are the variation, sample size, and mean accuracy for the model in question champions. we specifically choose our null hypothesis to be that the ranger model with 4 features preforms better than the other model, and our alternative hypothesis to be that the ranger model preforms at the same level or worse $(\mu_1 \leq \mu_0)$. We choose a significance level of $0.05$, as it is standard.
```{r Model Comparison Chart, echo = F}


bestModels <- crossdf %>% na.omit() %>%
  group_by(Model) %>%
  slice_max(Accuracy, n=3) %>%
  mutate(Model = str_to_title(str_replace(Model, '_', ' ')),
         `Standard Deviation` = round(sd, digits = 4),
         WinRate = as.numeric(str_detect(Feature, "WinRate")),
         PickRate = as.numeric(str_detect(Feature, "PickRate")),
         BanRate = as.numeric(str_detect(Feature, "BanRate")),
         Role = as.numeric(str_detect(Feature, "Role")),
         Accuracy = round(Accuracy, digits = 4)) %>%
  select(-Feature, -featurenum, -sd) %>%
  ungroup()


bestModels %>% group_by(Model) %>% slice_max(Accuracy, n=2) %>%
  unique() %>% arrange(desc(Accuracy)) %>%
  ungroup() %>%
  mutate(meanDiff = max(Accuracy) - Accuracy, 
         z = meanDiff/(sqrt(`Standard Deviation`^2/10 + 0.0161^2/10)), 
         pval = round(pt(q=z, df = 9, lower.tail=F), 4)) %>%
  rename(`T statistic` = z,`Distance from $\\mu_0$` =  meanDiff, `P Value` = pval) %>%
  kbl(caption = "Model Comparisons with $\\mu_0 = 0.7504$") %>%
  kable_classic(full_width = F, html_font = "cambria")
```
|      We can see from the table that the only model that has a p-value above $0.05$ is the SVM radial with all features. This means that the increase in accuracy between the two models cannot be said to be non-random, and we don’t have evidence to conclude that our ranger model outperforms the SVM radial model. Therefore, we choose our SVM radial model, for the reasons mentioned above.

```{r, eval = F, echo = F, echo = F}
#Model Fitting
#Deprecated wont work!
grid <- expand.grid(mtry = c(6,9,12), splitrule=c("extratrees", "gini"), min.node.size=c(1,3,6,10))

fitControl <- trainControl(method='CV', 
                           number = 5, 
                           classProbs = TRUE,
                           verboseIter=FALSE)

rf_fit <- train(Tier ~ .,
             data=leagueTrain,
             method="ranger",
             tuneGrid=grid,
             trControl = fitControl, importance="impurity"
)

saveRDS(rf_fit, "RF_Fit.rds")
```

```{r, eval = F, echo = F, warning = F, echo = F}

grid <- expand.grid(C = seq(0, 10, length = 20))


fitControl <- trainControl(method='CV', 
                           number = 5, 
                           verboseIter=FALSE)

svmlin_fit <- train(Tier ~ .,
                 data = leagueTrain,
                 method="svmLinear",
                 trControl = fitControl,
                 tuneGrid = grid)

saveRDS(svmlin_fit, "SVMLin_Fit.rds")
```

```{r, eval = F, echo = F, warning = F}
#Hypertuning parameters
#Algorithm is a greedy alg i came up with that goes as follows:
#1. Test with parameters, find the best fit, if it worst than the last then we are done, save the model
#2. Otherwise, we are not done. Take C and Sigma for the best fit of the last set of params, create an interval around them of [C - delta, C + delta], where both Delta are the distance (Cmax - Cmin)/5, and the same for Sigma
#3. If the new C interval < 0.1 in length, we are done because we have gone far enoguh. Save the model
#4. Repeat until complete.

#NOTE THAT THERE MAY BE A BUG WHERE IF WE OVERTRAIN THE MODEL (last was better than all current) WE DON'T GET THE ORIGINAL MODEL BACK!!!! POSSIBLY WANT TO FIX THIS BBG <3 But could always analyze and see cmax - cmin /2 for last C value
#Also likely has a memory leak since none of the models are locally defined. 

#improvement idea to fix both issues : Define train function that returns either previous best C and best sigma OR new best C and best sigma? Locally defined since function and if we can keep best C and sigma we can train a model on those specifically!

grid <- expand.grid(C = seq(0, 5, length = 5), sigma = seq(0, 0.1, length = 5))

cmin = 0
cmax = 5
sigmamin = 0
sigmamax = 0.1
done = F
last_acc = 0

fitControl <- trainControl(method='CV', 
                           number = 5, 
                           verboseIter=F)
while (done == F) {
  svmrad_fit <- train(Tier ~ .,
                   data = select(leaguedf, c(WinRate, PickRate, BanRate, Role, Tier)),
                   method="svmRadial",
                   trControl = fitControl,
                   tuneGrid = grid)
  best_fit <- svmrad_fit$results %>%
    slice_max(Accuracy, n=1) 
  print(best_fit)
  
  
  if (last_acc > best_fit$Accuracy[1]) {
    print("Best fit Lower then last Accuracy")
    done = T
  }
  else {
    if (best_fit$C[1] == cmax | best_fit$C[1] == cmin) {
      if (best_fit$C[1] == cmax) {
        cmax = cmax + (cmax - cmin)/2
        cmin = cmax - (cmax - cmin)/2
      }
      if (best_fit$C[1] == cmin & (cmin - (cmax - cmin) / 2) > 0) {
        cmax = cmin + (cmax - cmin) / 2
        cmin = cmin - (cmax - cmin) / 2
      }
    }
    else {
      cmin <- svmrad_fit$results %>%
        filter(C < best_fit$C) %>%
        slice_max(C, n = 1)
      cmin <- cmin$C[1]
      
      cmax <- svmrad_fit$results %>%
        filter(C > best_fit$C) %>%
        slice_min(C, n = 1)
      cmax <- cmax$C[1]
      
      
      
    }
    
    if (best_fit$sigma[1] == sigmamax | best_fit$sigma[1] == sigmamin) {
      if (best_fit$sigma[1] == sigmamax) {
      sigmamin = sigmamax - (sigmamax - sigmamin) / 2
      sigmamax = sigmamax + (sigmamax - sigmamin) /2
      }
      if (best_fit$sigma[1] == sigmamin & (sigmamin - (sigmamax - sigmamin)/2) > 0) {
        sigmamin = sigmamin - (sigmamax - sigmamin)/2
        sigmamax = sigmamin + (sigmamax - sigmamin)/2
      }
    }
    else {
      sigmamin = svmrad_fit$results %>%
        filter(sigma < best_fit$sigma) %>%
        slice_max(sigma, n = 1)
      sigmamin = sigmamin$sigma[1]
      
      sigmamax = svmrad_fit$results %>%
        filter(sigma > best_fit$sigma) %>%
        slice_min(sigma, n =1)
      sigmamax = sigmamax$sigma[1]
      }
    if (abs(cmin - cmax) < 0.01) {
      print("Within 0.01")
      print(abs(cmin - cmax))
      done = T      }
    else {
      
      grid <- expand.grid(C = seq(cmin, cmax, length = 5), sigma = seq(sigmamin, sigmamax, length = 5))
      last_acc <- best_fit$Accuracy[1]
      print(paste("New cmin is: ", cmin, "New cmax is:", cmax,  sep = ' '))
      }
    }
}
svmrad_fit <- train(Tier ~ .,
                    data  = select(leaguedf, c(WinRate, PickRate, BanRate, Role, Tier)),
                    method= "svmRadial",
                    trControl = fitControl,
                    tuneGrid = grid)
#saveRDS(svmrad_fit, "SVMRad_Fit.rds")
```

```{r, echo = F, eval = F}
svmrad_fit <- train(Tier ~ .,
                    data = select(leaguedf, c(WinRate, PickRate, BanRate, Role, Tier)),
                    method = "svmRadial",
                    trControl = fitControl,
                    tuneGrid = grid <- expand.grid(C = 4.6875, sigma = 0.10625))
```

```{r, echo = F}
#rf_fit <- readRDS("RF_Fit.rds")
#svmlin_fit <- readRDS("SVMLin_fit.rds")
svmrad_fit <- readRDS("SVMrad_fit.rds")
```

```{r, echo  = F, eval = F, echo = F}

MakeIsoSurface <- function(PlottingData, role) {
      
  #Prepariing Data
  
  
  #Create contour isosurface
  contour.list = lapply(PlottingData[,-c(4,5)], function(x) seq(min(x), max(x), len=50))
  contour = expand.grid(contour.list) %>%
    mutate(Role = role)
  contour.pred = predict(svmrad_fit, newdata = contour, decision.values = T)
  contour.df = fct_recode(contour.pred, 
    '1' = 'D',
    '2' = 'C',
    '3' = 'B',
    '4' = 'A',
    '5' = 'S',
    '6' = 'God'
  )

  
  contour.df = as.numeric(as.character(contour.df))
  contour.df = array(contour.df, dim = rep(50, 3))

  return (list(contour.list, contour.df))
}
MakeContourPlot <- function(contour.df, contour.list, PlottingData, role) {

  mfrow3d(2,3)
  for (i in 1:6) {
    #Create the actual isosurface from data
    surface = computeContour3d(
      vol = contour.df,
      maxvol = max(contour.df), 
      level = i,
      contour.list$WinRate,
      contour.list$PickRate,
      contour.list$BanRate) #LEVEL CONTROLS WHAT TIER WE WANT TO VIEW
    
    next3d()
    #Plot surface overlayed with datapoints
    plot3d(PlottingData[-c(4,5)], col= PlottingData$Tier, tag = role)
    triangles3d(x = surface[,1], y = surface[,2], z = surface[,3], color = i, tag = role) 
    title3d(main = as.character(tiers[i]), tag = role)
  }
  return (rglwidget())
}

```

### Visualization

```{r, echo = T, output = F, eval = F, echo = F}

tiers = c("D", "C", "B", "A", "S", "God")
roles = c("Top","Jungle", "Mid", "Adc", "Support")

PlottingData <- leaguedf %>%
    select(c(WinRate, PickRate, BanRate, Role, Tier)) %>%
    mutate(Tier = fct_recode(Tier, 
      '1' = 'D',
      '2' = 'C',
      '3' = 'B',
      '4' = 'A',
      '5' = 'S',
      '6' = 'God'
    )) %>%
    mutate(Tier = as.numeric(as.character(Tier)))


open3d()
#TOP
Plotting <- PlottingData %>%
  filter(Role == "Top")

subtop <- subsceneInfo()$id
dataList <- MakeIsoSurface(Plotting, "Top")
contour.list <- dataList[1][[1]]
contour.df <- dataList[2][[1]]

#Make one surface for each plot and save.

Topplot <-MakeContourPlot(contour.df, contour.list, Plotting, "Top")

#Now take each surface and generate a plot, the same way a loop would, but save it to a unique variable


#Repeat the steps every single time.
#MID
Plotting <- PlottingData %>%
  filter(Role == "Mid")

submid <- subsceneInfo()$id
dataList <- MakeIsoSurface(Plotting, "Mid")
contour.list <- dataList[1][[1]]
contour.df <- dataList[2][[1]]

Midplot <- MakeContourPlot(contour.df, contour.list, Plotting, "Mid")

#ADC

Plotting <- PlottingData %>%
  filter(Role == "Adc")

subadc <- subsceneInfo()$id
dataList <- MakeIsoSurface(Plotting, "Adc")
contour.list <- dataList[1][[1]]
contour.df <- dataList[2][[1]]

Adcplot <- MakeContourPlot(contour.df, contour.list, Plotting, "Adc")

#SUPPORT

Plotting <- PlottingData %>%
  filter(Role == "Support")

subsupport <- subsceneInfo()$id
dataList <- MakeIsoSurface(Plotting, "Support")
contour.list <- dataList[1][[1]]
contour.df <- dataList[2][[1]]

Supportplot <- MakeContourPlot(contour.df, contour.list, Plotting, "Support")


#JUNGLE
Plotting <- PlottingData %>%
  filter(Role == "Jungle")

subjungle <- subsceneInfo()$id
dataList <- MakeIsoSurface(Plotting, "Jungle")
contour.list <- dataList[1][[1]]
contour.df <- dataList[2][[1]]

Jungleplot <- MakeContourPlot(contour.df, contour.list, Plotting, "Jungle")

close3d()
#There is behind the scenes work here with Flexdashboard used to create our widget below. If you want to see it, go to https://github.com/DhAiBt/STOR-320-Group-13 and look at FinalPaper/ContourPlot.nb :-)
```

```{r Contour-Plot, fig.height = 5, fig.width  = 10, echo = F}
path <- "ContourPlot.html"
doc <- paste(readLines(path), " \n")
htmltools::tags$iframe(srcdoc = doc,
            width = "100%",
            style="height: 60vh;",
            scrolling = 'no')
```

```{r Heatmap, echo = F}
pred = predict(svmrad_fit, leaguedf)
cm <- confusionMatrix(pred, leaguedf$Tier, dnn = c("Prediction", "Actual"))

plt <- as.data.frame(cm$table) %>%
  group_by(Actual) %>%
  mutate(Percent = Freq / sum(Freq))

cm_plot <- ggplot(plt, aes(Prediction,Actual, fill=Percent)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Prediction",y = "Actual", title = "Confusion Matrix", guide = "Precision") + theme_minimal() + theme(legend.position = "bottom")

cm_plot
```

|      Now that we have chosen a model, we train a new one fitted to the entire dataset to be used in our analysis.

|      Next, we want to visualize the model to see 1) if there is evidence of major overfitting and 2) get a better understanding of how the model works and trends within our dataset. The contour plots pictured above show the boundary lines that our model draws between tiers, which partitions the feature space into mutually exclusive sections that it uses to classify each observation based on where it falls in the feature space. To best visualize this, we faceted the model by Role, and then created a unique plot for each tier. Our visualization shows us that there are some examples of overfitting, strange edges appearing out of certain values (for example towards the top of BanRate in some plots we can see that the models get very curved because its over fitted towards the outliers). We can also see that the ADC role has nearly linear lines drawn, which can show us part of the reason why the linear SVM models perform so well, while other roles tend to be more curved, hence why the radial model had better overall fits.
|      The next figure is a confusion matrix, which tells us where our model goes right and wrong in it's predictions. To create this, we predict the tiers of every observation in the original dataset using our model which is trained on the full dataset, and we then plot the counts of each prediction so that the actual tier is on the Y axis and the predicted tier is on the X axis. For example, if a specific observation is categorized as A Tier, and our model predicts it to be God Tier, it would go in the slot with God on the X axis and A on the Y axis (the box is labeled 1). From this chart, we can see that our model struggles the most with predicting B and C tier champions correctly, but there are also large numbers of mispredicted champions in A tier and S tier, often confused with each other. As expected from the K means analysis, we can see that the model predicts God Tier very well, however, it does not predict D tier well at all, which is surprising. This could point to the idea that features that were left out were important in the prediction of D tier champions. 

### Statistical Analysis

```{r TempChamps Tables, echo = F}
#Special Analysis just for TempChamps
#TODO: Fix table format so it isn't disgusting
leaguedf$Pred_svmrad = predict(svmrad_fit, leaguedf)
pretable <- leaguedf %>%
  group_by(Name, Role) %>%
  mutate(n = n()) %>%
  ungroup()  %>%
  mutate(Temp = n < 23, correct = Tier == Pred_svmrad) %>%
  group_by(Temp) %>%
  summarize(Acc = mean(correct), n = n(), sd = sd(correct)) %>%
  rename(Temporary = Temp, Accuracy = Acc, `Sample Size` = n, `Standard Deviation` = sd)

#Assume that the true probablility is correct. Then, we have H0 as Temp >= NonTemp, and H1 as Temp < NonTemp.

table <- tibble(
  NonTemporary = pretable[[1,2]], Temporary = pretable[[2,2]], NonTemporaryN = 4876, TemporaryN = 761, sdTemporary = pretable[[1,4]], sdNonTemporary = pretable[[2,4]]
) %>%
  mutate(diff  = NonTemporary - Temporary, z = diff/ (sqrt(sdTemporary^2 / TemporaryN + sdNonTemporary^2 / NonTemporaryN)), p = pnorm(q = z, lower.tail = F)) %>%
  rename(`Temporary Mean ($\\mu_1$)` = Temporary, `Non Temporary Mean ($\\mu_0$)` = NonTemporary, `Number of Temporary` = TemporaryN, `Number of NonTemporary` = NonTemporaryN, `Standard Deviation Temporary` = sdTemporary, `Standard Deviation Non Temporary` = sdNonTemporary, Difference = diff, `Z Score` = z, `P Value` = p) 
t = table[['Z Score']]
p = table[['P Value']]
diff = table[["Difference"]]
 
pretable %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "cambria")
```

|      We create the null hypothesis that the accuracy of temporary champions is greater than or equal to the accuracy of non temporary champions $(\mu_1 \geq \mu_0)$. Our alternative hypothesis is that the accuracy of temporary champions is less than the accuracy of nontemporary champions $(\mu_1 < \mu_0)$. We choose $p \leq 0.01$ to be a statistically significant measure, as we have large sample sizes with low variance. 

|      The difference between the two accuracy's is large, at `r round(diff, 3)`. To test for statistical significance, we use a Welsh's two-sample T test as described in the cross validation subsection. The T-statistic is `r round(t, 3)`, which corresponds to a P-value of $4 \cdot 10^{-6}$, and is statistically significant at the $0.01$ level. Therefore, we reject the null hypothesis, and conclude that our model performs worse on temporary champions compared to nontemporary champions.

# CONCLUSION

|      The first question is, “Can we see how MetaSRC produces the tiers of champions through clustering methods”? We specifically used k-means clustering with 6 centroids and Euclidean distance as the metric for the “closeness” of observations. What we found is that the MetaSRC’s tiers are rather sporadic. We found that only the “God” and “D” tiers, the best and the worst tiers share similarities with the k-means clusters. Furthermore, the role rate between tiers produced by k-means clusters is a lot more spread out than that of MetaSRC. The second question is whether the accuracy of predicting the tiers of temporary champions and nontemporary champions is significantly different. We used stepwise regression and Boruta for feature selection. From stepwise regression, we learned that 8 is an optimal number of features to make a model because adding more features will result in diminishing returns. Boruta decided that `WinRate`, `PickRate`, `Role`, and `BanRate` are the most important features, with `Role` being “Top”, “Jungle”, “Mid”, “ADC”, and “Support.” There are 8 features in total. With these features, we decided that an SVM radial model is the best model after cross-validation compared to SVM linear and ranger. Our model accurately predicts the tier of the combination of champions and roles 74.22% of the time. The best ranger performs slightly better at a 75.04% accuracy at an immense cost of computational power and difficulty to visualize. There is a significant difference between the accuracy of predicting the tier of temporary champions (70.4%) and that of non-temporary champions (77.7%).		

|      K-means clustering shows that MetaSRC’s tiers are somewhat sporadic, which is unusual for metrics such as win rate and pick rate to not fully classify the champions into tiers the way MetaSRC did. However, the God tiers and D tiers do match with the results from K-means, which are expected because those 2 tiers are the superlative tiers that contain either the best champions in certain roles or the worst. `WinRate`, `PickRate`, `Role`, and `BanRate` being the most significant features is somewhat expected because those all reflect the success of a champion other than `Role`. Role` more or less dictates the conventionality of a pick; most of the time, being a conventional role for a champion yields better success in games. Although the K-means cluster is good at clustering the D tier, our model is not good at predicting whether a champion is D or C at an accuracy of 69% and 63.8% respectively. It is expected that the accuracy for predicting the tiers of temporary champions is smaller than that of non-temporary champions, as temporary champions can contain off-role or newly-released champions, which makes sense to be harder to predict. 

|      There is value in these results since League of Legends is a popular video game that every video-game player would have played sometime in their gaming journey. MetaSRC has been around for a long time as a website that provides stats on League of Legends champions. Knowing how the tiers on MetaSRC are constructed and whether the tiers are worth following will be important for new players and veteran players alike, as not everyone has a lot of time devoted to studying the game. Tiers from MetaSRC help players decide whether a champion is worth playing or picking up. Especially in higher ranks, META becomes increasingly important as players from higher brackets can excavate all the potential in a champion. By producing our own tiers with a model that utilizes features indicative of success, we show the inner workings of how the tiers on MetaSRC are constructed. Since we showed that temporary champions and non-temporary champions are so different that the tiers cannot fully apply accurately, players should be wary of those odd temporary picks and the tiers projected by MetaSRC. The future direction of this modeling is to create a continuous metric such as a score that reflects how good overall a champion is and compare it with the Score variable in MetaSRC. 

|      Something we could do better is to implement noise injections to prevent overtraining or overfitting in our model. Noise injection is when we add a randomly generated vector (error/noise) to a column/variable so that the model does not pick up particular random patterns in a data set, such that the model fits too well to that data set and does terribly at predictions for other observations. In terms of additional data, there could potentially be value in a "rank" variable indicative of how good players generally are playing a said champion within a given role. Because of disparate skill floors and ceilings, champions may perform differently in different ranks. Mechanical-intensive champions may require a lot more game knowledge to pull off, whereas simple champions are easy to play. 




```{r, eval=FALSE, echo = F}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Boundarydf = expand.grid(WinRate = seq(0.4, 0.6, length.out=30),
                         PickRate = seq(0.0, 0.5, length.out= 20), 
                         BanRate = seq(0.0, 0.5, length.out = 20), 
                         RoleRate = seq(0.0, 1, length.out=20),
                         KDA = seq(1, 3, length.out = 30),
                         Role = "Top", 
                         ID = c("Camille_Fighter"), 
                         Patch = 13) %>%
  mutate(Name = str_replace(ID, "_.*", ''),
         Class = str_replace(ID, ".*_", ''))
Boundarydf$tier <- predict(rf_fit, Boundarydf)
Boundarydf <- Boundarydf %>%
  group_by(WinRate, KDA) %>%
  summarize(Tier = Mode(tier)) %>%
  ungroup()

write.csv(Boundarydf, "BoundaryFrame.csv")


```


```{r, echo=FALSE, eval = F, echo = F} 
#CHange Eval after Frame is created
Boundarydf <- read_csv("BoundaryFrame.csv")
```

```{r, fig.width = 10, eval = F, echo = F}
#Visualization #1 for Ranger Model
acc_plot <- rf_fit$results %>%  
  mutate(mtry = as.character(mtry), min.node.size = as.character(min.node.size)) %>%
  mutate(mtry = factor(mtry, levels=c("6", "9", "12", "15"), ordered=TRUE),
         min.node.size = factor(min.node.size, levels = c("1","3", "6", "10"), ordered=TRUE)) %>%
  ggplot() + geom_tile(mapping = aes(x=mtry, y=min.node.size, fill=Accuracy)) + facet_wrap(splitrule ~.) + ggtitle("Accuracy for each value") + theme(legend.position = "bottom")

rf_pred <- predict(rf_fit, leagueTest)

cm <- confusionMatrix(rf_pred, leagueTest$Tier, dnn = c("Prediction", "Actual"))
plt <- as.data.frame(cm$table) %>%
  group_by(Actual) %>%
  mutate(Percent = Freq / sum(Freq))

plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
cm_plot <- ggplot(plt, aes(Prediction,Actual, fill=Percent)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Actual",y = "Prediction", title = "Confusion Matrix") +
        scale_y_discrete(labels=c('D', 'C', 'B', 'A', 'S', 'God')) +
        scale_x_discrete(labels=c('God', 'S', 'A', 'B', 'C', 'D')) + theme_minimal() + theme(legend.position = "bottom")

boundary_plot <- leaguedf %>%
  filter(Role == 'Top'& Patch == 23 & (Class == "Fighter"| Class == "Tank"))  %>%
  ggplot() + geom_point(mapping = aes(x = KDA, y = WinRate, color = Tier)) + geom_raster(data = Boundarydf, mappign = aes(x = KDA, y = WinRate, fill = Tier))
  
(acc_plot + cm_plot) /boundary_plot
```


```{r, message = F, warning = F, echo = F, eval = F}
leaguedf %>%
  select("Name", "PickBanRate", "WinRate", "Role", "RoleRate", "Class", "Patch") %>%
  filter(!(Class == "NULL")) %>%
  group_by(Role) %>%
  group_map( ~ plot_ly(data = .,
      x = ~ PickBanRate,
      y = ~ WinRate,
      color = ~ Class,
      text = ~ Name,
      frame = ~ Patch, 
      hoverinfo = "text",
      type = "scatter",
      mode = "markers", 
      marker = list(size = ~ RoleRate*5)
      ), .keep = TRUE) %>%
  subplot(nrows = 2, shareX = TRUE, shareY=TRUE, margin=0.03) %>%
  layout(showlegend = FALSE, title = 'Pick Ban Rate vs. Win Rate by Patch seperated by Role',
         plot_bgcolor='#e5ecf6', 
         xaxis = list( 
           zerolinecolor = '#ffff', 
           zerolinewidth = 2, 
           gridcolor = 'ffff'), 
         yaxis = list( 
           zerolinecolor = '#ffff', 
           zerolinewidth = 2, 
           gridcolor = 'ffff'),
         margin = 0.07) %>%
  layout(annotations = annotations)

```



```{r CorrelationFunc, echo = F, eval = F}


MakeCorrelationDf <- function(varname, outputname = varname) {
  head(leaguedf)
  tempdf <- leaguedf %>%
    pivot_wider(id_cols = c("Name", "Role"), names_from = "Patch", values_from = varname) %>%
    mutate(ID = paste(Name, Role, sep = ' ')) %>%
    select(-c(1, 2)) %>%
    na.omit()
  NameList <- tempdf$ID
  
  tempdf <- tempdf %>%
    t() %>%
    as_tibble() %>%
    filter(row_number() < n()) %>%
    mutate_if(is.character, as.numeric)

  
  colnames(tempdf) <- NameList
  tempdf <- as_tibble(cor(tempdf))
  rownames(tempdf) <- colnames(tempdf)
  tempdf <- tempdf %>%
    rownames_to_column(var = "Champion1") %>%
    gather(key = "Champion2", value = !!paste(outputname, "Correlation", sep = ''), -Champion1) %>%
    filter(!(.by = Champion1 == Champion2))
  
  return(tempdf)
}
```

```{r PbrCorrelation, echo  = F, warning = F, eval = F}
PbrCorrelation <- MakeCorrelationDf("PickBanRate", "Pbr") %>%
  group_by(Champion2) %>%
  arrange(PbrCorrelation) %>%
  mutate(label = case_when(
    row_number() <= 2 ~ str_to_title(str_replace(Champion1, '\\.', ' ')),
    row_number() > n() - 2 ~ str_to_title(str_replace(Champion1, '\\.', ' ')), # This adds a Space into the name where the . is  and uncapitalizes the second role
    Champion2 == "Tahm Kench.Support" & PbrCorrelation > 0.68 ~ "Senna Support", # This is an outlier so labeling is justified, especially since it helps show the part of the plot
    TRUE ~ as.character(NA)
  )) 

PbrCorrelation %>% filter(Champion2 %in% c("Tahm Kench Support", "Senna Support", "Ashe Adc")) %>% 
  ggplot(mapping = aes(x=Champion2, y = PbrCorrelation)) + 
  geom_boxplot() + 
  ggtitle("PBR Correlation Boxplot")+ 
  scale_x_discrete(labels = c("Ashe Adc", "Senna Support", "Tahm Kench Support")) + 
  labs(x = "", y = "Pick Ban Rate Correlation Coefficient", caption = "Minimum and Maximum corelation coefficients are annotated, as well as Senna Support for Tahm Kench Support in order\n to best visualize how the strength of certain counters, replacements, and synergies effect Pick Ban Rate.")  + 
  geom_text(aes(label = label), na.rm = TRUE, hjust = -0.1, size = 3, check_overlap = T) 
```


```{r Std winRate, echo = F, message = F, eval = F}
leaguedf %>% 
  group_by(Name) %>% 
  summarise(Mean_pick=mean(PickRate, na.rm = TRUE), Std_pick=sd(PickRate, na.rm=TRUE), Mean_win=mean(WinRate, na.rm = TRUE), Std_win=sd(WinRate, na.rm=TRUE)) %>% 
  arrange(desc(Mean_pick)) %>% 
  ggplot(aes(Mean_pick, Std_win)) + geom_point() + labs(x = "Mean Pick Rate", y = "Standard Deviation Win Rate", title = "Mean PickRate and Volatility of WinRate") + 
  coord_trans(x = 'log10', y = 'log10') + 
  geom_smooth(aes(x = Mean_pick, y = Std_win), method = 'lm', se = F) # THIS LOOKS NON LINEAR BUT IT IS LINEAR, ITS JUST ON A LOG SCALE!!!!

```



```{r, fig.height = 15, eval = F, echo = F}
plot1 <- leaguedf %>%
  filter(Name %in% c("Fiora", "Darius", "Garen", "Aatrox", "Jax"), Role == "Top") %>%
  ggplot() + geom_count(aes(x = as.factor(Patch), y = Name, size = PickRate, color = Name)) + labs(x = "Patch", y = "Name", title = "Pick Rate")

plot2 <- leaguedf %>%
  filter(Name %in% c("Fiora", "Darius", "Garen", "Aatrox", "Jax"), Role == "Top") %>%
  ggplot() + geom_count(aes(x = as.factor(Patch), y = Name, size = BanRate, color = Name)) + labs(x = "Patch", y = "Name", title = "Ban Rate")

plot3 <- leaguedf %>%
  filter(Name %in% c("Fiora", "Darius", "Garen", "Aatrox", "Jax"), Role == "Top") %>%
  ggplot() + geom_count(aes(x = as.factor(Patch), y = Name, size = WinRate, color = Name)) + labs(x = "Patch", y = "Name", title = "Win Rate")

(plot1 / plot2/ plot3) + plot_annotation(title = "Analysis of Staple Top Champions")

```
